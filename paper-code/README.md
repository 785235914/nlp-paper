Paper-Recurrence | Still work
========================

由于考虑到部分模型及思路想法的复现代码量比较大，集中放在一个仓库中不合适，所以这里将一些代码量较大的项目分离到单独的仓库中，并通过引用指向。

# Notice | 特别说明
**特别说明**：此处模型代码为纯模型结构的代码，且都经过本人训练、测试、评价使用无误的代码，在代码关键处我添加了必要的注释，可以放心使用。

目前我的想法是直接复现论文时，只上传模型结构的代码，这样有利于缩小项目体积（本项目毕竟不是代码项目）。当然，如果有对部分复现代码感兴趣的伙伴，想要提出交流或者想要我实现完整过程等，欢迎提Issue。

# Coder | 模型代码
1. [Transformer](https://github.com/DengBoCong/paper/blob/master/paper-code/transformer.py)
2. [Scheduled Sampling for Transformer](https://github.com/DengBoCong/paper/blob/master/paper-code/transformer.py)
3. [GPT2](https://github.com/DengBoCong/paper/blob/master/paper-code/gpt2.py)
4. [GPT2-TF2.3完整仓库](https://github.com/DengBoCong/GPT2-TF2.3)：使用GPT2以及TensorFlow2.3实现闲聊，后续更新PyTorch。
5. [Sequential Matching Network](https://github.com/DengBoCong/paper/blob/master/paper-code/smn.py)
6. [Seq-to-Seq base](https://github.com/DengBoCong/paper/blob/master/paper-code/seq2seq.py)
7. [Neural Belief Tracker](https://github.com/DengBoCong/paper/blob/master/paper-code/nbt.py)